# Performance

## Pitfalls of Abstraction

```{r yolo, echo=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
library(DBI)

myDriver <- 'SQL Server'
myServer <- '.\\snapman'
myDatabase <- 'Cab_Demo'
sqlchunk_connection <- dbConnect(odbc::odbc(),
                          Driver= myDriver,
                          Server = myServer,
                          Database = myDatabase,
                          Trusted_Connection='yes')

```

> Virtue is the golden mean between two vices, the one of excess and the other of deficiency.

-Aristotle

What Aristotle says about virtue also applies to abstraction.  If we had no abstraction life would be more challenging.  I don't need to know the internals of power steering to use a car.  The wheel becomes an interface to the complexities.  At the other extreme, a lack of fundamental knowledge causes problems.  The fact that combustion engines require gasoline is an implementation detail.  All drivers must understand this level of detail or they will quickly become hitchhikers.

The DBI package provides a layer of abstraction between the R user and the underlying database system.  Many R users have no need or desire to understand the internals of an RDBMS.  However, if they know the basics of databases, then they can deliver a more robust solution.

The R function dbCreateTable() from DBI allows the user to create a SQL table without defining column data types.  It's tedious to look at each column and figure out the most appropriate data type, but failure to do so can lead to future performance problems.  For example, suppose we use dbCreateTable() with the Yellow Cab data file.

```{r, echo=FALSE}
myDriver <- 'SQL Server'
myServer <- '.\\snapman'
myDatabase <- 'Cab_Demo'
library(DBI)  
 con <- dbConnect(odbc::odbc(),Driver= myDriver,Server = myServer
                   ,Database = myDatabase,Trusted_Connection='yes')
if(dbExistsTable(con, "yellow_trip_summary_model"))
  {dbRemoveTable(con , 'yellow_trip_summary_model')}
```

```{r,eval=FALSE}
url <- "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2018-01.csv"
download.file(url,"D:/Cab_Data/yellow_tripdata_2018-01.csv")
trips_fs<- read_csv('D:/Cab_Data/yellow_tripdata_2018-01.csv')

dbCreateTable(con,'yellow_trip_summary_model',trips_fs)
```

First, the dbCreateTable function generates a CREATE TABLE SQL statement.  Then the statement is executed against the database specified in connection argument.  A single line of R code replaces several lines of SQL code.

```{sql,connection = sqlchunk_connection,eval=FALSE}
CREATE TABLE yellow_trip_summary(
    [VendorID] [float] NULL,
    [tpep_pickup_datetime] [datetime] NULL,
    [tpep_dropoff_datetime] [datetime] NULL,
    [passenger_count] [float] NULL,
    [trip_distance] [float] NULL,
    [RatecodeID] [float] NULL,
    [store_and_fwd_flag] [varchar](255) NULL,
    [PULocationID] [float] NULL,
    [DOLocationID] [float] NULL,
    [payment_type] [float] NULL,
    [fare_amount] [float] NULL,
    [extra] [float] NULL,
    [mta_tax] [float] NULL,
    [tip_amount] [float] NULL,
    [tolls_amount] [float] NULL,
    [improvement_surcharge] [float] NULL,
    [total_amount] [float] NULL,
    [Hour_Range] [varchar](10) NULL,
    [Day] [char](3) NULL
) 
```

The generated code might meet the requirements, but it has some flaws.  First, is the FLOAT data type appropriate for every numeric data type?  Float expresses approximate numeric values with a default storage size of eight bytes.  Eight bytes are wasteful for several of the columns.  For example, the payment_type column could be defined as TINYINT because only five classes [1,2,3,4,5] of payment types exist. 

```{sql,connection = sqlchunk_connection,eval=TRUE }
SELECT payment_type,count(1)
  FROM [Cab_Demo].[dbo].[yellow_trip_summary_partitioned]
  group by payment_type
```

As long as the number of payment type classes stays below 256, TINYINT will work.  Using TINYINT, with a storage size of one byte would save space and could help prevent decimal codes from entering the database.  Should users be able to enter a payment code of 4.2?  They technical could with a float data type.

When performance or accuracy matters then developers should specify the data types.  I created another table with the smallest possible data type for each column.  At 100 million rows, this table design requires 44% less disk space compared to the first table.

```{sql,connection = sqlchunk_connection,output.var = "DataTypes",eval=FALSE }
CREATE TABLE [dbo].[yellow_trip_summary](
    [VendorID] [tinyint] NULL,
    [tpep_pickup_datetime] [datetime] NULL,
    [tpep_dropoff_datetime] [datetime] NULL,
    [passenger_count] [tinyint] NULL,
    [trip_distance] [smallint] NULL,
    [RatecodeID] [tinyint] NULL,
    [store_and_fwd_flag] [char](1) NULL,
    [PULocationID] [smallint] NULL,
    [DOLocationID] [smallint] NULL,
    [payment_type] [tinyint] NULL,
    [fare_amount] [decimal](8, 2) NULL,
    [extra] [decimal](8, 2) NULL,
    [mta_tax] [decimal](8, 2) NULL,
    [tip_amount] [decimal](8, 2) NULL,
    [tolls_amount] [decimal](8, 2) NULL,
    [improvement_surcharge] [decimal](8, 2) NULL,
    [total_amount] [decimal](8, 2) NULL,
    [Hour_Range] [varchar](10) NULL,
    [Day] [char](3) NULL
)
```


The size on disk for the first table is 16.1 GB and the second table is 8.9 GB. A difference of 7.2 GB might not seem like a big deal.  After all, disk space is relatively cheap. But disk shouldn't be the only consideration for design.  The 7.2 GB now takes up room in memory.  

 * Memory
  * Network
  * Backups
  * Indexes
  * Replication



## Indexing

> If your data fits in memory there is no advantage to putting it in a database: it will only be slower and more frustrating.[@hwickham2019]

It can be a hassle working with databases on ad-hoc data analysis projects, but in the context of Shiny Application development, a database can help the app scale.

For example, imagine we are tasked with building a shiny dashboard to analyze New York City Yellow Taxi Trip public data located at https://on.nyc.gov/2tn71Qq.  If we leave the data in a CSV format, then how does it perform compared to data imported to a database?

We can compare performance using the microbenchmark package.  The following code downloads a month worth of taxi cab data and imports the file into a SQL table.  I use a T-SQL bulk command because it is orders of magnitude faster at importing data then using an R function or single SQL insert statement.

```{r,echo=TRUE,eval=FALSE,message=FALSE}
url <- "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2018-01.csv"
download.file(url,"D:/Cab_Data/yellow_tripdata_2018-01.csv")

packages <- list('odbc','DBI','tidyverse','microbenchmark')
aa <-lapply(packages, require, character.only = TRUE,quietly = TRUE)

con <- dbConnect(odbc(),Driver = 'SQL Server',Server = '.\\snapman'
                 , Database = 'Cab_Demo', trusted_connection = TRUE)


dbExecute(con,"CREATE TABLE [dbo].[yellow_tripdata_2018-01](
    [VendorID] [smallint] NULL,
    [tpep_pickup_datetime] [datetime] NULL,
    [tpep_dropoff_datetime] [datetime] NULL,
    [passenger_count] [smallint] NULL,
    [trip_distance] [real] NULL,
    [RatecodeID] [smallint] NULL,
    [store_and_fwd_flag] [varchar](1) NULL,
    [PULocationID] [smallint] NULL,
    [DOLocationID] [smallint] NULL,
    [payment_type] [smallint] NULL,
    [fare_amount] [real] NULL,
    [extra] [real] NULL,
    [mta_tax] [real] NULL,
    [tip_amount] [real] NULL,
    [tolls_amount] [real] NULL,
    [improvement_surcharge] [real] NULL,
    [total_amount ] [real] NULL
) ON [PRIMARY]")

dbExecute(con,"BULK INSERT [dbo].[yellow_tripdata_2018-01]
FROM 'D:\Cab_Data\yellow_tripdata_2018-01.csv'
WITH (FORMAT = 'CSV')"

```


With the data loaded, we can run a simulate a sample workload.  The following dplyr queries perform a filter and count of the rows.  One query is pointed at the CSV and the other at the SQL table.

```{r,message=FALSE,eval=FALSE}
fs <- function() csvquery <- trips_fs %>% 
    filter(tpep_dropoff_datetime >= '2018-01-02 07:28:00'
          ,tpep_dropoff_datetime <= '2018-01-02 07:30:00') %>%
    summarise(pcount= n())
  rs<- suppressMessages(compute(csvquery))

db <- function() sqltable <- trips_db %>% 
    filter(tpep_dropoff_datetime >= '01-02-2018 13:28'
           ,tpep_dropoff_datetime <= '01/02/2018 13:30')  %>%
    summarise(pcount= n())
  rs1<-suppressMessages(compute(sqltable))
  
  
```

The benchmark function reveals a performance difference between the choice of storage.  The CSV query had a lower median execution time.  Thus, it would seem the claim that putting data in the database would make the analysis run slower is vindicated. 

```{r boxplot2,eval=FALSE}
rs<-microbenchmark(db(),fs(),times = 100)
rs<-as.data.frame(rs)
rs$time <- rs$time/1000000
rs <- rs %>% filter(time < 1000) # Remove outliers on upp
ggplot(rs, aes(x=expr, y=time, fill=expr)) +
 geom_boxplot(alpha=0.4) +
    theme(text = element_text(size=20))

```

![](C:/Users/mshar/OneDrive/Documents/R_UG_Demo/ShinyDB_Book/Assets/boxplot2-1.png)

But the story changes when we add an index to the table.

```{r,eval=FALSE,message=FALSE}
ms<-dbExecute(con,
'CREATE NONCLUSTERED INDEX nc_yellow_trip
 ON [yellow_tripdata_2018-01](tpep_dropoff_datetime)'
 )
```

The function pointed at the SQL table now outperforms the CSV function.

![](C:/Users/mshar/OneDrive/Documents/R_UG_Demo/ShinyDB_Book/Assets/boxplot3-1.png)

The db() function works faster because the query uses the index to seek into the data.  Without an index, the query had no choice but to scan every single data page to satisfy the filter condition.

Indexing a table is a bit of art and a bit of science.  Creating the best index can be challenging even for veteran DBAs.  Columns used for filtering, like the query in the workload above, make good candidates for an index.  Imagine you need to look up information about George Washington in a history textbook.  There are two physical ways to execute this query: 1) Read the contents of all the pages or 2) Go to the index in the back of the book and read just the pages associated with George Washington.  The second method seeks the relevant pages and takes far less time than reading through all the pages.  A SQL index based on a filtering column is roughly analogous to the book example.





