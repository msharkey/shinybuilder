---
title: "My Shiny Story"
author: "Matt Sharkey"
date: "7/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## From IT to Data Science

In March of 2018 a colleuge approached me about an internal job posting.  His team was looking for a Data Engineer to support the budding Data Science team.  Although I was content with my current position, the opputunity seemed intreguing.  As a Database Administrator I had worked with Data Science on mostly operational task e.g. granting data permissions, answering performance related questions, setting up auditing, configuring encryption.  So I had a general understanding of what they were up to. The Data Science team was ramping up and needed more development resources dedicated to data aquistion.  Essentially, they needed an ETL developer.  I took the interview and was offered the position.  After a cost benefit analysis and a tough discussion with my current manager I decided the best route forward was to take the job.

Within in the fist two weeks of starting a team memember resigned.  Some of his projects, including development of a web-app were transitioned to me.  Intially, It annoyed me that I had just interviewed for an ETL developer role and was now being asked to take over a half-completed web-app in a stack I didn't have a lot of experiance in. In fact, I didn't have a lot of web-app experiance in general as I've been a database specialist my entire career.  So after several tough discussions with my manager and the application stakeholders I made the decision to re-write the app from scratch.

## R you serious?

I decided to re-write the app using R with a SQL Server backend.  At that point I had about 2000 hours of R coding experiance from grad school and personal training.  Also, my coworkers understood the internals of R like a DBA understands the internals of SQL Server.  If I had any gaps in my knowledge they could help.  Using SQL Server as the backend was the most logical choice.  Most of my expertise is database programing on the SQL Server data platform.  My plan was to push as much of the business logic and processing on the database as possible because I could debug SQL much faster than R.  I was aware that R had several packages that abstracted some of the major compoments of web development.  For example, I used the Shiny package to generate about 95% of my HTML, CSS, and Javascript.  Again, this was perfect for someone like me whose skills lie outside of front-end web development.  Instead of writting HTML I called functions from the Shiny library.  For example, if I wanted "Hello World!" to appear in the browser I would need to declare it inside of &lt;h2> tags.  But with Shiny I called the h2() function.


```{r,eval=FALSE,echo=TRUE}
<h2>Hello World!</h2>
```

<h2>Hello World!</h2>


```{r,eval=TRUE,echo=TRUE}
library(shiny)

h2('Hello World!')

```

User control could be generated from Shiny functions as well.  The html for date input was verbose for me but a Shiny function felt more intuitive.


```{r,eval=FALSE,echo=TRUE}
dateRangeInput("date", 
               strong("Date range"),
               start = "2007-01-01", 
               end = "2017-07-31",
               min = "2007-01-01", 
               max = "2017-07-31"
               )
```



The dateRangeInput() function above generates the following html code.

```{r,eval=FALSE,echo=TRUE}
<div id="date">
<label class="control-label" for="date"> <strong>Date range</strong>
</label><div class="input-daterange input-group">
<input class="input-sm form-control" type="text"
data-date-week-start="0" data-date-format="yyyy-mm-dd"
data-date-start-view="month" data-min-date="2007-01-01" 
data-max-date="2017-07-31"data-initial-date="2007-01-01" 
data-date-autoclose="true"/><span class="input-group-addon">
to </span> <input  type="text" data-date-language="en" 
data-date-week-start="0" data-date-format="yyyy-mm-dd" 
data-min-date="2007-01-01" data-max-date="2017-07-31"/>
  </div></div>
```

I needed more than a date input for my app.  The Shiny gallery app helped me find a menu of inputs.  I saw functional examples as well as the code used to create them.  

https://shiny.rstudio.com/gallery/


## App Archecture

I started to get a feel for application Archecture after looking at demo apps and watching the tutorial video on the Shiny portal https://shiny.rstudio.com/tutorial/.  The main components were the UI, the Server Side code, and the database.  

![](./Images/ShinyArc.png)

The code below provides the minumun components required for a Shiny database app.  A developer can use it as template for starting a new project. The first line after the library functions creates a database connection object to SQL Server.  The connection object depends on functions from the DBI and odbc library.  The next line of code creates a UI object, then the server object, and the final line runs takes the UI and server object and runs the app.

```{r,eval=FALSE}
library(shiny)
library(DBI)
library(odbc)
con <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')

my_ui <- fluidPage()

my_server <- function(input, output) {}

shinyApp(ui = my_ui, server = my_server)

```



### UI

The UI contained functions related to web browser display e.g. the date input control above.  The site format, theme, user input controls, server outputs like graphs and tables are all managed in the UI. The best way to understand the UI is to look at an example.  I wrote a simple Shiny App to view system CPU utilization over the last four hours.  The UI contains two important functions - one for accepting user input in the form of slider filter and the other displays the CPU plot output.

//![] App screen shot goes here

We'll build the app one step at a time.  First we'll tackle the input slider.  The slider helps users determine how far back the plot should display data up to a max of the last 256 minutes. we create the slider with the sliderInput function.  The required aurguments are the inputId, the label, the minumum value, maximum value and defualt value.  It's important to note that the value for inputId must be unique for each input object. All input objects require a inputId so they can be referenced in the Server object.  Finally, we'll wrap the slider in a side bar to give the page an organized look.


```{r,eval=FALSE}
library(shiny)

con <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')

my_ui <- fluidPage(sidebarPanel(sliderInput("cpu_slider","Minutes Back",0,256,256))

my_server <- function(input, output) {}

shinyApp(ui = my_ui, server = my_server)

```


Now we need to add a function for our plot.  The function is named plotOutput() and it currently references a non existant plot named "cpuPlot".  We will build cpuPlot in the server object.  The app appearance hasn't changed however, there is now space reserved for the plot in the main pannel on the web page.


```{r,eval=FALSE}
library(shiny)

con <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')

my_ui <- fluidPage(sidebarPanel(sliderInput("cpu_slider","Minutes Back",0,256,256))
                   , mainPanel(plotOutput("cpuPlot")))

my_server <- function(input, output) {}

shinyApp(ui = my_ui, server = my_server)

```

## Server and Database Intergration

The plot uses data extracted from SQL Server via a T-SQL query.  I'm using a slighty modified script from Glenn Berry.

```{r,echo=FALSE}
library(odbc)
library(DBI)
sqlcon <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')
```

```{sql,eval=TRUE,connection=sqlcon}
 DECLARE @ts_now bigint = (SELECT cpu_ticks/(cpu_ticks/ms_ticks) FROM sys.dm_os_sys_info WITH (NOLOCK)); 
   
  SELECT TOP(256)   DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) AS [Event_Time] ,
                 100 - SystemIdle AS [CPU_Utilization]
  FROM (SELECT record.value('(./Record/@id)[1]', 'int') AS record_id, 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') 
              AS [SystemIdle], 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') 
              AS [SQLProcessUtilization], [timestamp] 
        FROM (SELECT [timestamp], CONVERT(xml, record) AS [record] 
              FROM sys.dm_os_ring_buffers WITH (NOLOCK)
              WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR' 
              AND record LIKE N'%<SystemHealth>%') AS x) AS y 
  ORDER BY record_id DESC OPTION (RECOMPILE);
  
```

The query works great but now we need to integrate it with our slider input.  The goal is that the plot time range can be controlled by the slider input.  The default value for the input is 256 minutes, so when the app starts it will pull back the maximum range.  When the user moves the slider the query will need to re-run and pull back the range specified.  To ensure the query re-runs each time the slider input is moved we must wrap the query inside an output function in the server object.


```{r,eval=FALSE}
library(odbc)
library(DBI)


my_ui <- fluidPage(sidebarPanel(sliderInput("cpu_slider","Minutes Back",0,256,256))
                   , mainPanel(plotOutput("cpuPlot")))

my_server <- function(input, output) {
  
  output$cpuPlot <- renderPlot({
  con <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')
  myquery <- paste0("DECLARE @ts_now bigint = (SELECT cpu_ticks/(cpu_ticks/ms_ticks) FROM sys.dm_os_sys_info WITH (NOLOCK)); 
   
  SELECT TOP(256)   DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) AS [Event_Time] ,
                 100 - SystemIdle AS [CPU_Utilization]
  FROM (SELECT record.value('(./Record/@id)[1]', 'int') AS record_id, 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') 
              AS [SystemIdle], 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') 
              AS [SQLProcessUtilization], [timestamp] 
        FROM (SELECT [timestamp], CONVERT(xml, record) AS [record] 
              FROM sys.dm_os_ring_buffers WITH (NOLOCK)
              WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR' 
              AND record LIKE N'%<SystemHealth>%') AS x) AS y 
              Where DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) >= DATEADD(minute,-",input$cpu_slider,",Getdate())
  ORDER BY record_id DESC OPTION (RECOMPILE);
  ")
 mydata <- dbGetQuery(con,myquery)

 dbDisconnect(con)
  })
}

shinyApp(ui = my_ui, server = my_server)

```

The myquery variable holds the query text combined with current value in slider input.  When the app starts the current value of the sliderInput cpu_slider is concatenated with the query then executed against the server with the dbGetQuery function.  The contents of the query are stored in mydata.  Each time the user moves the slider input it invalidates the contents of the output object and triggers all the code contained within the brackets of the renderPlot function to re-run.  This means we get an updated query and a new data set loaded to mydata.

Now we have our data but no function to generate a plot.  The ggplot2 library provides the app with a line plot function.

```{r,eval=FALSE}

library(ggplot2)

 ggplot(mydata,aes(Event_Time,CPU_Utilization)) + geom_line()
 
```


//Recap here in presentation

## Performance Tunning and Optimization

### Security

We have a basic functioning app but it has a few design flaws.  The most pressing issue is concatenating user input with the query string.  As a rule of thumb, never mix trusted data (in this case the query string) with untrusted data (the user input).  If a malicous use how figured out how to bypass the input control they in theory could execute arbituary scripts against the SQL Server.  we can mitigate the risk by parametrizing the query.  The sqlinterpolate function from DBI allows us to paramertize.


```{r,eval=FALSE}
myquery <- "DECLARE @ts_now bigint = (SELECT cpu_ticks/(cpu_ticks/ms_ticks) FROM sys.dm_os_sys_info WITH (NOLOCK)); 
  SELECT TOP(256)   DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) AS [Event_Time] ,
                 100 - SystemIdle AS [CPU_Utilization]
  FROM (SELECT record.value('(./Record/@id)[1]', 'int') AS record_id, 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') 
              AS [SystemIdle], 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') 
              AS [SQLProcessUtilization], [timestamp] 
        FROM (SELECT [timestamp], CONVERT(xml, record) AS [record] 
              FROM sys.dm_os_ring_buffers WITH (NOLOCK)
              WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR' 
              AND record LIKE N'%<SystemHealth>%') AS x) AS y 
              Where DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) >= DATEADD(minute,-?cpu_slider_param,Getdate())
  ORDER BY record_id DESC OPTION (RECOMPILE);"

myquery_param <- sqlInterpolate(con,myquery,.dots =c(cpu_slider_param <- input$cpu_slider))
mydata <- dbGetQuery(con,myquery_param)

```

With text inputs I'd take input validation one more step and implement whitelists.  For example, a user email input control must follow a certain regex pattern.  The pseudo code below populates the emailistwhitelist variable with a regex pattern of an email I found on stackoverflow.  The if block checks if the input matches the pattern.  If it doesn't then the app will not pass the input to the database.


```{r,eval=FALSE}
emailwhitelist <- "^[[:alnum:].-_]+@[[:alnum:].-]+$"

if(!is.na(str_match(usr_email, emailwhitelist))){
  #Run query
} else
{
  # Reject input
}
```


### Connection Management

The app seems a little slugish.  If performance isn't acceptable then the fist place to turn is profvis.  Profvis provides line by line execution metrics for R code.  We'll run the app inside the profvis function and then interact with the application as a normal user would.  After we close the session a bar chart reporting memory consumed and execution time appears next to each function in our app.  Also, a flame graph shows exection time at each stage of the callstack. The flame graph is built on a mapping library and can be controlled (pan, zoom in, zoom out) like a Google map.

```{r,echo=FALSE,eval=FALSE}
library(profvis)

profvis(runApp("C:/Users/mshar/OneDrive/Old/Documents/R_UG_Demo/01-SQL_SAT_Presentation/slowapp.R"))
```

![](./Images/ProfvisData.JPG)

![](./Images/Flame.JPG)


The profile session reveals dbConnect() accounts for most of the exection time.  The app makes a connection to our database each time the user adjusts the slider input.  Instead building a connection and closing it for each query execution we can establish a pool and let R handle opening and closing connections.  We need two minor app code changes to use connection pooling.  First we need to reference the pool library and then replace the dbConnect() with dbPool().  We don't need to close the pool when the application is running so dbDisconnect() gets removed. Finally, we move dbPool() outside of the server object completely.

```{r,eval = FALSE}
library(pool)

pool <- dbPool(drv = odbc(),Driver = mydriver,Server = myserver,Database = myDatabase,Trusted_Connection='yes')

  results <- dbGetQuery(pool,myquery)

```

In addtion to making our session faster, using pool also helps the app scale.  Pool dynamically opens and closes connections as needed.  So as users start to increase they may use an existining connection from the pool.  


### Error Handling

Error handling can improve user experiance and prevent the app from crashing.  If an error occurs the app should do something like fix the issue automatically or alert the user.  When we increase app complexity we increase the need for error handling.  Adding a relational database to the app is an example of increasing complexity. In addition to all the potential bugs in Shiny, packages, and O.S. of the webserver we now have to account for all the potential bugs in the database software and O.S. of the database server. For example, I'll simulate a database outage by forcing the database offline while the app is running.

```{sql,connection=sqlcon}
ALTER Database Test Set SINGLE_USER WITH ROLLBACK IMMEDIATE;
ALTER Database Test Set OFFLINE;
```

When the user adjusts the slider the query attempts to re-run but fails.  R displays an error containing the query text where the plot should be.  This error is problematic because the error message means nothing to th average user and second I might not want implementation details like table  names exposed to end users.

![](./Images/ErrorMessage.JPG)

We can avoid this issue with a tryCatch function wrapped around the query execution.  If an error occurs during executionof the code within the tryCatch it passes execution off to the error function.

```{r,eval=FALSE}
      tryCatch({
        results <- dbGetQuery(pool,myquery)
        j<- ggplot(results,aes(Event_Time,CPU_Utilization))
        j+ geom_line()
        },error =function(e) {
            showModal(modalDialog(
                h5('There was an error.  Please contact the system admin.')
            )
            )
        })
       
```

Now instead of red error messages the user get's a modal box with instructions.

![](./Images/ModalError.JPG)

The error function stores error details from the R session like the error message.  It's possible to write error specific messages or events.  A good example of error specific handlong is re-submitting the query in the event of a deadlock.  This code looks for the SQL Server specific error message 1205 for deadlocks.  If the error message contains "1205" then the function re-submits the query three times.

```{r,eval=FALSE}
tryCatch({
    dbGetQuery(con,myquery)
}
,error = function(e){
  if(grep('1205',e$message)==1){
    while (x<4){
      tryCatch({
        dbGetQuery(con,myquery)
        break
        }
      ,error=function(e){
      x<<- x+1
      })
    }
  }
})
```



### Load Testing

The app has reached acceptable performance for one user session.  But how does the app handle multiple user sessions?  We can find out using a load test.  We need shinyload package, shinycannon program and Java for running load tests.

Similar to profvis the fist step of a load test is recording a typical user session.  In most cases this session would be recorded on an application deployed to a webserver, however for demonstartion I record the session with app hosted on my local machine.  I start two instances of Rstudio. One runs the Shiny app and the other runs the recording function record_session().

```{r,eval=FALSE}
shinyloadtest::record_session('http://127.0.0.1:6696/')
```

The record_session function launches the app in a browser.  Once the browser closes any action taken during the session is written to recording.log in a timestamped folder in the working directory.  The path to recording.log makes up one the first required positional argument in shinycannon.  The second required argument is the app URL.  We'll see how the app performs with 50 users for two minutes of load. The following code runs in a terminal.

```{sh,eval=FALSE}
$ shinycannon recording.log http://127.0.0.1:6696/ --workers 50 --loaded-duration-minutes 2
```

After the load test finishes we can view the results throught shinyloadtest.

```{r,eval=FALSE,message=FALSE, warning=FALSE}

df <- shinyloadtest::load_runs("50workers" = "./test-logs-2019-07-22T03_21_19.997Z")

shinyloadtest::shinyloadtest_report(df, "run1.html")

```

```{r echo=FALSE}
htmltools::includeHTML("gantt-run-50workers.svg")
```

The X Axis represents elasped time and the Y represents displays each session.  Sessions contained within the black dots indicate when all user sessions were executing.  At 10 concurrent users session performance pluments.  The 11th app user would be waiting seconds plots to render.  Interupting a user workflow for two seconds will not make for happy users. We need a go faster button.


### Plot and Plan Caching

We already know from the profivs session that the two most expensive functions aside from the dbConnect were renderPlot and dbGetQuery.  We can't tune the query much but we can remove the OPTION(recompile) query hint.  This will enable plan caching on the SQL server. Instead of recompiling the plan each time our app will look it up in cache.

```{r,eval=FALSE}
myquery <- "DECLARE @ts_now bigint = (SELECT cpu_ticks/(cpu_ticks/ms_ticks) FROM sys.dm_os_sys_info WITH (NOLOCK)); 
  SELECT TOP(256)   DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) AS [Event_Time] ,
                 100 - SystemIdle AS [CPU_Utilization]
  FROM (SELECT record.value('(./Record/@id)[1]', 'int') AS record_id, 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') 
              AS [SystemIdle], 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') 
              AS [SQLProcessUtilization], [timestamp] 
        FROM (SELECT [timestamp], CONVERT(xml, record) AS [record] 
              FROM sys.dm_os_ring_buffers WITH (NOLOCK)
              WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR' 
              AND record LIKE N'%<SystemHealth>%') AS x) AS y 
              Where DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) >= DATEADD(minute,-?cpu_slider_param,Getdate())
  ORDER BY record_id DESC
  --OPTION (RECOMPILE);"## Disabling hint so plan caching can occur
```


we can also enable plot caching with minor app code changes.  The renderCachedPlot function replaces render plot and we specify a cache key after the renderCachedPlot closing curly brace.  The natural choice is the input slider.  When a user selects the same input they, in theory, would be requesting the same plot.  Rather than re-render the same plot, the app can retrivie it from cache.

```{r,eval=FALSE}
  },cacheKeyExpr = {input$cpu_slider})

```

There's a design flaw with this app.  The data returned by the query is not deterministic because the data frame is a sliding window of the last 256 minutes of CPU utilization.  We would need to persist the results of the query in a timestamped history table to prevent wrong plot results.  The user input control also needs updated to either a time stamp or free text numeric field.  For the purpuse of the demo we will skip the backend change for now as we are focused on the impact of plot caching.











