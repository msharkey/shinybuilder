---
title: "My Shiny Story"
author: "Matt Sharkey"
date: "7/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## From IT to Data Science

In March of 2018 a colleuge approached me about an internal job posting.  His team was looking for a Data Engineer to support the budding Data Science team.  Although I was content with my current position, the opputunity seemed intreguing.  As a Database Administrator I had worked with Data Science on mostly operational task e.g. granting data permissions, answering performance related questions, setting up auditing, configuring encryption.  So I had a general understanding of what they were up to. The Data Science team was ramping up and needed more development resources dedicated to data aquistion.  Essentially, they needed an ETL developer.  I took the interview and was offered the position.  After a cost benefit analysis and a tough discussion with my current manager I decided the best route forward was to take the job.

Within in the fist two weeks of starting a team memember resigned.  Some of his projects, including development of a web-app were transitioned to me.  Intially, It annoyed me that I had just interviewed for an ETL developer role and was now being asked to take over a half-completed web-app in a stack I didn't have a lot of experiance in. In fact, I didn't have a lot of web-app experiance in general as I've been a database specialist my entire career.  So after several tough discussions with my manager and the application stakeholders I made the decision to re-write the app from scratch.

## R you serious?

I decided to re-write the app using R with a SQL Server backend.  At that point I had about 2000 hours of R coding experiance from grad school and personal training.  Also, my coworkers understood the internals of R like a DBA understands the internals of SQL Server.  If I had any gaps in my knowledge they could help.  Using SQL Server as the backend was the most logical choice.  Most of my expertise is database programing on the SQL Server data platform.  My plan was to push as much of the business logic and processing on the database as possible because I could debug SQL much faster than R.  I was aware that R had several packages that abstracted some of the major compoments of web development.  For example, I used the Shiny package to generate about 95% of my HTML, CSS, and Javascript.  Again, this was perfect for someone like me whose skills lie outside of front-end web development.  Instead of writting HTML I called functions from the Shiny library.  For example, if I wanted "Hello World!" to appear in the browser I would need to declare it inside of &lt;h2> tags.  But with Shiny I called the h2() function.


```{r,eval=FALSE,echo=TRUE}
<h2>Hello World!</h2>
```

<h2>Hello World!</h2>


```{r,eval=TRUE,echo=TRUE}
library(shiny)

h2('Hello World!')

```

User control could be generated from Shiny functions as well.  The html for date input was verbose for me but a Shiny function felt more intuitive.


```{r,eval=FALSE,echo=TRUE}
dateRangeInput("date", 
               strong("Date range"),
               start = "2007-01-01", 
               end = "2017-07-31",
               min = "2007-01-01", 
               max = "2017-07-31"
               )
```



The dateRangeInput() function above generates the following html code.

```{r,eval=FALSE,echo=TRUE}
<div id="date">
<label class="control-label" for="date"> <strong>Date range</strong>
</label><div class="input-daterange input-group">
<input class="input-sm form-control" type="text"
data-date-week-start="0" data-date-format="yyyy-mm-dd"
data-date-start-view="month" data-min-date="2007-01-01" 
data-max-date="2017-07-31"data-initial-date="2007-01-01" 
data-date-autoclose="true"/><span class="input-group-addon">
to </span> <input  type="text" data-date-language="en" 
data-date-week-start="0" data-date-format="yyyy-mm-dd" 
data-min-date="2007-01-01" data-max-date="2017-07-31"/>
  </div></div>
```

I needed more than a date input for my app.  The Shiny gallery app helped me find a menu of inputs.  I saw functional examples as well as the code used to create them.  

https://shiny.rstudio.com/gallery/


## App Archecture

I started to get a feel for application Archecture after looking at demo apps and watching the tutorial video on the Shiny portal https://shiny.rstudio.com/tutorial/.  The main components were the UI, the Server Side code, and the database.  

//![](/Users/mshar/OneDrive/Old/Documents/R_UG_Demo/04-Images/ShinyArc.png)

The code below provides the minumun components required for a Shiny database app.  A developer can use it as template for starting a new project. The first line after the library functions creates a database connection object to SQL Server.  The connection object depends on functions from the DBI and odbc library.  The next line of code creates a UI object, then the server object, and the final line runs takes the UI and server object and runs the app.

```{r,eval=FALSE}
library(shiny)
library(DBI)
library(odbc)
con <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')

my_ui <- fluidPage()

my_server <- function(input, output) {}

shinyApp(ui = my_ui, server = my_server)

```



### UI

The UI contained functions related to web browser display e.g. the date input control above.  The site format, theme, user input controls, server outputs like graphs and tables are all managed in the UI. The best way to understand the UI is to look at an example.  I wrote a simple Shiny App to view system CPU utilization over the last four hours.  The UI contains two important functions - one for accepting user input in the form of slider filter and the other displays the CPU plot output.

```{r, echo=FALSE, warning=FALSE,message=FALSE}

library(shiny)
library(DBI)
library(odbc)
library(ggplot2)
library(pool)
library(profvis)


mydriver <-  "SQL Server"
myserver <- ifelse(Sys.info()["nodename"]=="INFRA035",".",".\\snapman")
myDatabase <- 'Test'


pool <- dbPool(drv = odbc(),Driver = mydriver,Server = myserver,Database = myDatabase,Trusted_Connection='yes')

shinyApp(

ui <- fluidPage(

    titlePanel("CPU Last 4 hours"),
    sidebarLayout(
        sidebarPanel(
            sliderInput("minutesRange","Minutes ago",min=0,max=256,value=256)
        ),

        mainPanel(
           plotOutput("cpuPlot")
        )
    )
),


server <- function(input, output) {
    
    output$cpuPlot <- renderPlot({

        query <- "DECLARE @ts_now bigint = (SELECT cpu_ticks/(cpu_ticks/ms_ticks) FROM sys.dm_os_sys_info WITH (NOLOCK)); 
                    
                    SELECT TOP(256) SQLProcessUtilization AS [SQL Server Process CPU Utilization], 
                                   SystemIdle AS [System Idle Process], 
                                   100 - SystemIdle - SQLProcessUtilization AS [CPU_Utilization], 
                                   DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) AS [Event_Time] 
                    FROM (SELECT record.value('(./Record/@id)[1]', 'int') AS record_id, 
                    			record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') 
                    			AS [SystemIdle], 
                    			record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') 
                    			AS [SQLProcessUtilization], [timestamp] 
                    	  FROM (SELECT [timestamp], CONVERT(xml, record) AS [record] 
                    			FROM sys.dm_os_ring_buffers WITH (NOLOCK)
                    			WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR' 
                    			AND record LIKE N'%<SystemHealth>%') AS x) AS y 
                    	Where DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE())>=DATEADD(minute,- ?minRange, GETDATE())
                    ORDER BY record_id DESC OPTION (RECOMPILE);"
        
        myquery <- sqlInterpolate(pool,query,.dots=c(minRange=input$minutesRange))
       
        tryCatch({
        results <- dbGetQuery(pool,myquery)
        },error =function(e) {
            showModal(modalDialog(
                h5('There was an error')
            )
            )
        })
        j<- ggplot(results,aes(Event_Time,CPU_Utilization))
        j+ geom_line()
        
       
    })
   
},


  options = list(height = 500)
)
```

We'll build the app one step at a time.  First we'll tackle the input slider.  The slider helps users determine how far back the plot should display data up to a max of the last 256 minutes. we create the slider with the sliderInput function.  The required aurguments are the inputId, the label, the minumum value, maximum value and defualt value.  It's important to note that the value for inputId must be unique for each input object. All input objects require a inputId so they can be referenced in the Server object.  Finally, we'll wrap the slider in a side bar to give the page an organized look.


```{r,eval=FALSE}
library(shiny)

con <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')

my_ui <- fluidPage(sidebarPanel(sliderInput("cpu_slider","Minutes Back",0,256,256))

my_server <- function(input, output) {}

shinyApp(ui = my_ui, server = my_server)

```

```{r, echo=FALSE,message=FALSE,warning=FALSE}
shinyApp(

  ui <- fluidPage(sidebarPanel(sliderInput("cpu_slider","Minutes Back",0,256,256))),

  server = function(input, output) { },

  options = list(height = 500)
)
```

Now we need to add a function for our plot.  The function is named plotOutput() and it currently references a non existant plot named "cpuPlot".  We will build cpuPlot in the server object.  The app appearance hasn't changed however, there is now space reserved for the plot in the main pannel on the web page.


```{r,eval=FALSE}
library(shiny)

con <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')

my_ui <- fluidPage(sidebarPanel(sliderInput("cpu_slider","Minutes Back",0,256,256))
                   , mainPanel(plotOutput("cpuPlot")))

my_server <- function(input, output) {}

shinyApp(ui = my_ui, server = my_server)

```

## Server an Database Intergration

The plot uses data extracted from SQL Server via a T-SQL query.  I'm using a slighty modified query originally written by Glenn Berry.

```{r,echo=FALSE}
library(odbc)
library(DBI)
sqlcon <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')
```

```{sql,eval=TRUE,connection=sqlcon}
 DECLARE @ts_now bigint = (SELECT cpu_ticks/(cpu_ticks/ms_ticks) FROM sys.dm_os_sys_info WITH (NOLOCK)); 
   
  SELECT TOP(256)   DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) AS [Event_Time] ,
                 100 - SystemIdle AS [CPU_Utilization]
  FROM (SELECT record.value('(./Record/@id)[1]', 'int') AS record_id, 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') 
              AS [SystemIdle], 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') 
              AS [SQLProcessUtilization], [timestamp] 
        FROM (SELECT [timestamp], CONVERT(xml, record) AS [record] 
              FROM sys.dm_os_ring_buffers WITH (NOLOCK)
              WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR' 
              AND record LIKE N'%<SystemHealth>%') AS x) AS y 
  ORDER BY record_id DESC OPTION (RECOMPILE);
  
```

The query works great but now we need to integrate it with our slider input.  The goal is that the plot time range can be controlled by the slider input.  The default value for the input is 256 minutes, so when the app starts it will pull back the maximum range.  When the user moves the slider the query will need to re-run and pull back the range specified.  To ensure the query re-runs each time the slider input is moved we must wrap the query inside an output function in the server object.


```{r,eval=FALSE}
library(odbc)
library(DBI)


my_ui <- fluidPage(sidebarPanel(sliderInput("cpu_slider","Minutes Back",0,256,256))
                   , mainPanel(plotOutput("cpuPlot")))

my_server <- function(input, output) {
  
  output$cpuPlot <- renderPlot({
  con <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.\\snapman',Database = 'Test' ,Trusted_Connection='yes')
  myquery <- paste0("DECLARE @ts_now bigint = (SELECT cpu_ticks/(cpu_ticks/ms_ticks) FROM sys.dm_os_sys_info WITH (NOLOCK)); 
   
  SELECT TOP(256)   DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) AS [Event_Time] ,
                 100 - SystemIdle AS [CPU_Utilization]
  FROM (SELECT record.value('(./Record/@id)[1]', 'int') AS record_id, 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') 
              AS [SystemIdle], 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') 
              AS [SQLProcessUtilization], [timestamp] 
        FROM (SELECT [timestamp], CONVERT(xml, record) AS [record] 
              FROM sys.dm_os_ring_buffers WITH (NOLOCK)
              WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR' 
              AND record LIKE N'%<SystemHealth>%') AS x) AS y 
              Where DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) >= DATEADD(minute,-",input$cpu_slider,",Getdate())
  ORDER BY record_id DESC OPTION (RECOMPILE);
  ")
 mydata <- dbGetQuery(con,myquery)

 dbDisconnect(con)
  })
}

shinyApp(ui = my_ui, server = my_server)

```

The myquery variable holds the query text combined with current value in slider input.  When the app starts the current value of the sliderInput cpu_slider is concatenated with the query then executed against the server with the dbGetQuery function.  The contents of the query are stored in mydata.  Each time the user moves the slider input it invalidates the contents of the output object and triggers all the code contained within the brackets of the renderPlot function to re-run.  This means we get an updated query and a new data set loaded to mydata.

Now we have our data but no function to generate a plot.  The ggplot2 library provides the app with a line plot function.

```{r,eval=FALSE}
library(odbc)
library(DBI)
library(ggplot2)

my_ui <- fluidPage(sidebarPanel(sliderInput("cpu_slider","Minutes Back",0,256,256))
                   , mainPanel(plotOutput("cpuPlot")))

my_server <- function(input, output) {
  
  output$cpuPlot <- renderPlot({
  con <- dbConnect(drv = odbc(),  Driver = 'Sql Server',Server = '.',Database = 'Test' ,Trusted_Connection='yes')
  myquery <- paste0("DECLARE @ts_now bigint = (SELECT cpu_ticks/(cpu_ticks/ms_ticks) FROM sys.dm_os_sys_info WITH (NOLOCK)); 
   
  SELECT TOP(256)   DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) AS [Event_Time] ,
                 100 - SystemIdle AS [CPU_Utilization]
  FROM (SELECT record.value('(./Record/@id)[1]', 'int') AS record_id, 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') 
              AS [SystemIdle], 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') 
              AS [SQLProcessUtilization], [timestamp] 
        FROM (SELECT [timestamp], CONVERT(xml, record) AS [record] 
              FROM sys.dm_os_ring_buffers WITH (NOLOCK)
              WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR' 
              AND record LIKE N'%<SystemHealth>%') AS x) AS y 
              Where DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) >= DATEADD(minute,-",input$cpu_slider,",Getdate())
  ORDER BY record_id DESC OPTION (RECOMPILE);
  ")
 mydata <- dbGetQuery(con,myquery)
 dbDisconnect(con)
 
 ggplot(mydata,aes(Event_Time,CPU_Utilization)) + geom_line()
 

  })
}

shinyApp(ui = my_ui, server = my_server)

```


//Recap here in presentation

## Performance Tunning and Optimization

### Security

We have a basic functioning app but it has a few design flaws.  The most pressing issue is concatenating user input with the query string.  As a rule of thumb, never mix trusted data (in this case the query string) with untrusted data (the user input).  If a malicous use how figured out how to bypass the input control they in theory could execute arbituary scripts against the SQL Server.  we can mitigate the risk by parametrizing the query.  The sqlinterpolate function from DBI allows us to paramertize.


```{r,eval=FALSE}
myquery <- "DECLARE @ts_now bigint = (SELECT cpu_ticks/(cpu_ticks/ms_ticks) FROM sys.dm_os_sys_info WITH (NOLOCK)); 
  SELECT TOP(256)   DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) AS [Event_Time] ,
                 100 - SystemIdle AS [CPU_Utilization]
  FROM (SELECT record.value('(./Record/@id)[1]', 'int') AS record_id, 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') 
              AS [SystemIdle], 
              record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') 
              AS [SQLProcessUtilization], [timestamp] 
        FROM (SELECT [timestamp], CONVERT(xml, record) AS [record] 
              FROM sys.dm_os_ring_buffers WITH (NOLOCK)
              WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR' 
              AND record LIKE N'%<SystemHealth>%') AS x) AS y 
              Where DATEADD(ms, -1 * (@ts_now - [timestamp]), GETDATE()) >= DATEADD(minute,-?cpu_slider_param,Getdate())
  ORDER BY record_id DESC OPTION (RECOMPILE);"

myquery_param <- sqlInterpolate(con,myquery,.dots =c(cpu_slider_param <- input$cpu_slider))
mydata <- dbGetQuery(con,myquery_param)

```

With text inputs I'd take input validation one more step and implement whitelists.  For example, a user email input control must follow a certain regex pattern.  The pseudo code below populates the emailistwhitelist variable with a regex pattern of an email I found on stackoverflow.  The if block checks if the input matches the pattern.  If it doesn't then the app will not pass the input to the database.


```{r,eval=FALSE}
emailwhitelist <- "^[[:alnum:].-_]+@[[:alnum:].-]+$"

if(!is.na(str_match(usr_email, emailwhitelist))){
  #Run query
} else
{
  # Reject input
}
```

### Connection Management

The app seems a little slugish.  If performance isn't acceptable then the fist place to turn is profvis.  Profvis provides line by line execution metrics for R code.  We'll run the app inside the profvis function and then interact with the application as a normal user would.  After we close the session a bar chart reporting memory consumed and execution time appears next to each function in our app.  Also, a flame graph shows exection time at each stage of the callstack. The flame graph is built on a mapping library and can be controlled (pan, zoom in, zoom out) like a Google map.

```{r,echo=FALSE,eval=FALSE}
library(profvis)

profvis(runApp("C:/Users/mshar/OneDrive/Old/Documents/R_UG_Demo/01-SQL_SAT_Presentation/slowapp.R"))
```

The profile session reveals dbConnect() accounts for most of the exection time.  The app makes a connection to our database each time the user adjusts the slider input.  Instead building a connection and closing it for each query execution we can establish a pool and let R handle opening and closing connections.  We need two minor app code changes to use connection pooling.  First we need to reference the pool library and then replace the dbConnect() with dbPool().  We don't need to close the pool when the application is running so dbDisconnect() gets removed. Finally, we move dbPool() outside of the server object completely.

```{r,eval = FALSE}
library(pool)

pool <- dbPool(drv = odbc(),Driver = mydriver,Server = myserver,Database = myDatabase,Trusted_Connection='yes')

  results <- dbGetQuery(pool,myquery)

```

```{r,echo=FALSE,eval=FALSE}
library(profvis)

profvis(runApp("C:/Users/mshar/OneDrive/Old/Documents/R_UG_Demo/01-SQL_SAT_Presentation/slowapp.R"))
```

In addtion to making our session faster, using pool also helps the app scale.  Pool dynamically opens and closes connections as needed.  So as users start to increase they may use an existining connection from the pool.  

### Load Testing





