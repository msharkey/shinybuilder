---
title: "Building Your First Shiny Database Application"
author: "Matthew Sharkey | msharkey3434@gmail.com"
date: "`r Sys.Date()`"
bibliography: ["citation.bib"]
output:
  xaringan::moon_reader:
    #css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, echo=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
library(DBI)
library(dbplyr)
library(tidyverse)
library(kableExtra)
library(microbenchmark)
myDriver <- 'SQL Server'
myServer <- '.\\snapman'
myDatabase <- 'Cab_Demo'
sqlchunk_connection <- dbConnect(odbc::odbc(),
                          Driver= myDriver,
                          Server = myServer,
                          Database = myDatabase,
                          Trusted_Connection='yes')
comp <- Sys.info()
if(comp["nodename"]=="OMC-5CG6151GZF"){
  cab_fileloc <- "C:/Users/MaSharkey/Cab_Data/yellow_tripdata_2018-12.csv"} else {
  cab_fileloc <- "D:/Cab_data/yellow_tripdata_2018-01.csv"
  }
```



# About Me

Matthew Sharkey | Data Engineer, HDR

--

Presentation write up on blog [www.hintank.com](https://www.hinttank.com/)

--

Demo Application on github (update with github link)

---


# Agenda

1. RDBMS Use Cases

2. Building and Managing Connections

3. Secure Design

4. Performance & Optimization


---

# Why use an RDBMS?


Potiential performance gains
--

- Indexing

--

- Parallelism

---

## File vs Table Query

<br>
> If your data fits in memory there is no advantage to putting it in a database: it will only be slower and more frustrating.

<br>
Hadley Wickham, RStudio, Edgar Ruiz. 2019. Dbplyr: A dplyr Back End for Databases. https://CRAN.R-project.org/package=dbplyr.

<br>

--

- frustrating --> Possibly

--

- slower --> It Depends

---

## File Storage vs Table

```{r,echo=FALSE,message=FALSE}
packages <- list('odbc','DBI','tidyverse','microbenchmark')
aa <-lapply(packages, require, character.only = TRUE,quietly = TRUE)

con <- dbConnect(odbc(),Driver = 'SQL Server',Server = '.\\snapman'
                 , Database = 'Cab_Demo', trusted_connection = TRUE)
#drop index for demo
dm <- dbExecute(con,"DROP INDEX IF EXISTS nc_yellow_trip
              ON [yellow_tripdata_2018-01] ")
```
 Same data but one stored as CSV and one stored as table 
 <div>
 Data source:NYC Yellow Taxi Trips https://on.nyc.gov/2tn71Qq
 
--
```{r,message=FALSE,echo=FALSE}
if(exists("trips_fs") == FALSE)
{
trips_fs <- read_csv(cab_fileloc)
trips_db <- tbl(con, "yellow_tripdata_2018-01")
}
```

```{r,message=FALSE,eval=FALSE}
trips_fs <- read_csv(cab_fileloc)
trips_db <- tbl(con, "yellow_tripdata_2018-01")
```
--
```{r,message=FALSE}
fs <- function(){csvquery<- trips_fs %>% 
    filter(tpep_dropoff_datetime >= '2018-01-02 07:28:00'
          ,tpep_dropoff_datetime <= '2018-01-02 07:30:00') %>%
    summarise(pcount= n())
  rs<- suppressMessages(compute(csvquery))}
```
--
```{r}
db <- function(){sqltable <- trips_db %>% 
    filter(tpep_dropoff_datetime >= '01-02-2018 13:28'
           ,tpep_dropoff_datetime <= '01/02/2018 13:30')  %>%
    summarise(pcount= n())
  rs1<-suppressMessages(compute(sqltable))}
```


---

## Benchmarking

File storage is faster

```{r}
microbenchmark(db(),fs(),times = 10)
```



---

## Indexing

Indexing makes a difference

--
```{r,echo=FALSE,message=FALSE}
ms<-dbExecute(con,
'CREATE NONCLUSTERED INDEX nc_yellow_trip
 ON [yellow_tripdata_2018-01](tpep_dropoff_datetime)'
 )
```

```{r,eval=FALSE}
dbExecute(con,
'CREATE NONCLUSTERED INDEX nc_yellow_trip
 ON [yellow_tripdata_2018-01](tpep_dropoff_datetime)'
 )
```
--
```{r,message=FALSE}
microbenchmark(db(),fs(),times = 10)
```

```{r,echo=FALSE}
dbDisconnect(con)
```


--

The tables have turned.

---

## Other Reasons to use a RDBMS

- Stability

--

- Security

--

- Transactions (ACID)

--

- Change Tracking

--

- But it's not the right tool for every job!

---
## So Help Me Codd!


<br>
<center>
>IT should never forget that technology is a means to an end, and not an end in itself. Technologies must be evaluated individually in terms of their ability to satisfy the needs of their respective users. IT should never be reluctant to use the most appropriate interface to satisfy users' requirements. Attempting to force one technology or tool to satisfy a particular need for which another tool is more effective and efficient is like attempting to drive a screw into a wall with a hammer when a screwdriver is at hand: the screw may eventually enter the wall but at what cost?

<br>
<center>
E.F. Codd, et al. 1998. Providing Olap to User-Analysts: An It Mandate. https://bit.ly/2Okyz2H.


---

class: inverse, center, middle

# Building and Managing Connections


---





```{r commonCon, eval=TRUE, warning=FALSE}
library(DBI)
myDriver <- 'SQL Server'
myServer <- '.\\snapman'
myDatabase <- 'Cab_Demo'
myconnection <- dbConnect(odbc::odbc(),
                          Driver= myDriver,
                          Server = myServer,
                          Database = myDatabase,
                          Trusted_Connection='yes')


```

--

```{r execQuery, eval=TRUE, warning=FALSE}

dbGetQuery(myconnection,"Select GetDate()")
dbDisconnect(myconnection)

```

---


## Common Connection Problems

.pull-left[
1. Wrong connection string format 
    
1. Misspelled something

1. Permissions

1. Driver

1. Wrong port number

1. Firewall
]

.pull-right[
<center>
<iframe src="https://giphy.com/embed/Ym0zs6Ms4vgwU" width="380" height="227" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/mr-robot-rami-malek-its-so-good-Ym0zs6Ms4vgwU"></a></p>
</center>
]


---


```{r lotsofCons, eval=TRUE,echo=FALSE}
for (i in 1:5)
{
myconnection <- dbConnect(odbc::odbc(),Driver= myDriver
                          ,Server = myServer,Database = myDatabase
                          ,Trusted_Connection='yes')
date <- dbGetQuery(myconnection,paste("Select GetDate() as mydate
                                      ,\'leakedquery\' as c1,",i))
}

```


## Leaked Connections

```{sql,connection = sqlchunk_connection,output.var = "leakedcns", echo=FALSE}
SELECT 
    DB_NAME(p.dbid) as DBName, 
    program_name as Program,CPU,memusage,status,SPID,nt_username
FROM sys.sysprocesses p
Cross apply sys.dm_exec_sql_text(p.sql_handle) d
WHERE  program_name = 'Rstudio'
and text like '%leakedquery%' and text not like '%DB_NAME%'

```

```{r leakedcons, warning= FALSE,echo=FALSE}
knitr::kable(leakedcns, format = 'html')
dbDisconnect(myconnection)
nn <- gc()
```


---

## Manual vs Pool

```{r pool, eval=TRUE,echo=FALSE}
library(pool)
queries <- c("SELECT Getdate()","Select Getdate()","Select Getdate()","Select Getdate()")

dbconnectworkload <- function() {
 for(i in 1:length(queries)){
   con <- dbConnect(odbc::odbc(),Driver= myDriver,Server = myServer
                   ,Database = myDatabase,Trusted_Connection='yes')
  dbGetQuery(con,queries[i])
  dbDisconnect(con)
 }
}  

poolcon <- dbPool(odbc::odbc(),Driver= myDriver,Server = myServer
                  ,Database = myDatabase,Trusted_Connection='yes')

dbpoolworkload <- function() {
 for(i in 1:length(queries)){dbGetQuery(poolcon,queries[i])}
}

results <- microbenchmark(dbconnectworkload(),dbpoolworkload(),times = 10)



```

```{r,eval=FALSE}
  {{dbConnect}}(
    odbc::odbc(),Driver= myDriver,Server = myServer
                ,Database = myDatabase,Trusted_Connection='yes')
  dbGetQuery(con,query)
  dbDisconnect(con)  
```
--
```{r,eval=FALSE}
  {{dbPool}}(
     odbc::odbc(),Driver= myDriver,Server = myServer
                  ,Database = myDatabase,Trusted_Connection='yes')
```
--
```{r poolres, eval=TRUE}
microbenchmark(dbconnectworkload() ,dbpoolworkload(),times = 10)
```

--- 
---

class: inverse, center, middle

# Secure Design

---

## OWASP Top 10 - 2017

--

1. Injection
2. Broker Authentication
3. Sensitive Data Exposure
4. XML External Entities (XXE)
5. Broken Access Control
6. Security Misconfiguration
7. Cross-Site Script (XSS)
8. Insecure Deserialization
9. Using Components with Known Vulnerabilities
10. Insufficient Logging and Monitoring

---

## SQL Injection

```{r, out.width = "700px",echo=FALSE}
knitr::include_graphics("C:/Users/mshar/OneDrive/Documents/R_UG_Demo/sqlinjection.gif")
```


---


## Injection Demo

```{sql,connection = sqlchunk_connection,echo=FALSE}
DROP Table IF Exists dbo.Persons


CREATE TABLE dbo.Persons (
	email Varchar(100) NOT NULL PRIMARY KEY,
	jobtitle Varchar(50)
	)

	INSERT INTO dbo.Persons
	VALUES ('msharkey3434@gmail.com','Data Engineer'),
	       ('q2474554@nwytg.net','Analyst')


```

```{r, out.width = "700px",echo=FALSE}
knitr::include_graphics("C:/Users/mshar/OneDrive/Documents/R_UG_Demo/shinybuilderInjection.gif")
```

---

## Why Injection works

Unsecure

```{r,eval=FALSE}
dbGetQuery(myPool
    ,paste0("Insert into dbo.Persons 
            values ('",input$usr_email,"','",input$usr_title,"')"))
```

--

Better

```{r,eval=FALSE}
    params <- c(email = input$usr_email,title = input$usr_title)
    query <- "Insert into dbo.Persons values (?email,?title)"
    queryint <- sqlInterpolate(myPool, query, .dots = params)
    dbGetQuery(myPool, queryint)
```

---

## Whitelist

```{r,eval=FALSE}
{{emailwhitelist <- "^[[:alnum:].-_]+@[[:alnum:].-]+$"}}

if(!is.na(str_match(input$usr_email, emailwhitelist))){
 params <- c(email = usr_email,title = usr_title)
 query <- "Insert into dbo.Persons values (?email,?title)"
 queryint <- sqlInterpolate(myPool, query, .dots = params)
 dbGetQuery(myPool, queryint)
} else {stop("Not a valid email.")}
```

---

## SQL Injection Defense Recap

--

- Grant user least possible permissions

--

- Look out for dynamically generated code

--

- Whitelist Input

--

- Let app interact with interface, not implementation

---

class: inverse, center, middle

# Performance & Optimization

---


## Abstraction - We find virtue between excess and deficiency


```{r, echo=FALSE}
   con <- dbConnect(odbc::odbc(),Driver= myDriver,Server = myServer
                   ,Database = myDatabase,Trusted_Connection='yes')
if(dbExistsTable(con, "yellow_trip_summary_model"))
  {dbRemoveTable(con , 'yellow_trip_summary_model')}
```

```{r}
dbCreateTable(con,'yellow_trip_summary_model',trips_fs)
```

--

```{sql,connection = sqlchunk_connection,echo=FALSE,output.var = "DataTypes" }
Select c.name as Column_Name 
,t.name as Default_Type
,t2.name as Hand_Crafted
,Byte_Diff as Byte_Difference 
from sys.columns c
JOIN sys.types t
ON c.user_type_id = t.user_type_id
JOIN sys.columns cs
ON c.name = cs.name
and cs.object_id = object_id('yellow_trip_summary_partitioned')
JOIN sys.types t2
ON cs.user_type_id = t2.user_type_id
JOIN
(
Select 'VendorID' as name,7 as Byte_Diff UNION ALL
Select 'tpep_pickup_datetime' as name,0 as Byte_Diff UNION ALL
Select 'tpep_dropoff_datetime' as name,0 as Byte_Diff UNION ALL
Select 'passenger_count' as name,7 as Byte_Diff UNION ALL
Select 'trip_distance' as name,6 as Byte_Diff UNION ALL
Select 'RatecodeID' as name,7 as Byte_Diff UNION ALL
Select 'store_and_fwd_flag' as name,0 as Byte_Diff UNION ALL
Select 'PULocationID' as name,6 as Byte_Diff UNION ALL
Select 'DOLocationID' as name,6 as Byte_Diff UNION ALL
Select 'payment_type' as name,7 as Byte_Diff UNION ALL
Select 'fare_amount' as name,3 as Byte_Diff UNION ALL
Select 'extra' as name,3 as Byte_Diff UNION ALL
Select 'mta_tax' as name,3 as Byte_Diff UNION ALL
Select 'tip_amount' as name,3 as Byte_Diff UNION ALL
Select 'tolls_amount' as name,3 as Byte_Diff UNION ALL
Select 'improvement_surcharge' as name,3 as Byte_Diff UNION ALL
Select 'total_amount' as name,3 as Byte_Diff 
) ch
ON c.name = ch.name
Where c.object_id = object_id('yellow_trip_summary_model')
```

```{r , warning= FALSE,echo= FALSE}
knitr::kable(DataTypes,  format="html") %>% 
  kable_styling() %>%
  scroll_box(width = "100%", height = "500px")
```

---

## 67 Bytes Per Row?  So What?

--

- One year of data = 100 million rows

--

- 6.7 GB of wasted space, compounded by:

  * Memory
  * Network
  * Backups
  * Indexes
  * Replication
  
  
---

## Indexes - The Keys to Query Performance


Good columns for indexing

--

- Remember FPOC

--

F - Filtering (Columns used in joins and the filter/WHERE clause)

--

P - Partitioning (columns used in partitioning functions)

--

O- Ordering (ARRANGE/ORDER BY)

--

C- Covering (Any remaining column in the SELECT)


---

## Best index for this query?

```{r,eval= FALSE}
trips_db %>%
      mutate(Trip_Date = as.Date(tpep_dropoff_datetime))  %>% 
      filter(tpep_dropoff_datetime >= '01/01/2018',
             tpep_dropoff_datetime < '01/04/2018',
             trip_distance >= 0,
             trip_distance <= 5)  %>%
      group_by(Trip_Date) %>%
      summarise(Trip_Count= n())

```


--

- Per FPOC, start with tpep_dropoff_datetime and trip_distance

--

```{sql,eval=FALSE,connection=sqlchunk_connection}
CREATE INDEX ix_yellow_cab 
ON yellow_trip(tpep_dropoff_datetime,trip_distance)
```

--

- Test performance before and after index

- Indexing trade-off = faster reads, slower writes

---

## Demo

- Yellow Cab Dashboard

---

# Recap

- Use the right tool for the job

--

- Troubleshooting connections

--

- Preventing SQL Injection

--

- Peformance

--

More in-depth look @ hinttank.com

---

class: center, middle

# Thank You

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).


